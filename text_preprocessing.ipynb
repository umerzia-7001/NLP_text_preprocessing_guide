{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/beautifulsoup4-4.8.2-py3.7.egg (from bs4) (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from beautifulsoup4->bs4) (2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (2.3.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (46.0.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./Library/Python/3.7/lib/python/site-packages (from spacy) (1.18.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (7.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (4.45.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./Library/Python/3.7/lib/python/site-packages (from spacy) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./Library/Python/3.7/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./Library/Python/3.7/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./Library/Python/3.7/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./Library/Python/3.7/lib/python/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for re\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: mapping in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.1.6)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from mapping) (1.0.3)\n",
      "Requirement already satisfied: numpy in ./Library/Python/3.7/lib/python/site-packages (from mapping) (1.18.1)\n",
      "Requirement already satisfied: cvxpy in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from mapping) (1.1.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->mapping) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from pandas->mapping) (2.8.1)\n",
      "Requirement already satisfied: ecos>=2 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cvxpy->mapping) (2.0.7.post1)\n",
      "Requirement already satisfied: scipy>=1.1.0 in ./Library/Python/3.7/lib/python/site-packages (from cvxpy->mapping) (1.4.1)\n",
      "Requirement already satisfied: osqp>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cvxpy->mapping) (0.6.1)\n",
      "Requirement already satisfied: scs>=1.1.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from cvxpy->mapping) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->mapping) (1.14.0)\n",
      "Requirement already satisfied: future in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from osqp>=0.4.1->cvxpy->mapping) (0.18.2)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: unidecode in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (1.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: textblob in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.14.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in ./Library/Python/3.7/lib/python/site-packages (from gensim) (1.18.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in ./Library/Python/3.7/lib/python/site-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: requests in ./Library/Python/3.7/lib/python/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./Library/Python/3.7/lib/python/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./Library/Python/3.7/lib/python/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./Library/Python/3.7/lib/python/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./Library/Python/3.7/lib/python/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.7/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages')\n",
    "# importing libraries\n",
    "!pip3 install bs4\n",
    "!pip3 install nltk\n",
    "!pip3 install spacy\n",
    "!pip3 install re\n",
    "!pip3 install mapping\n",
    "!pip3 install unidecode\n",
    "!pip3 install textblob\n",
    "!pip3 install gensim\n",
    "#from bs4 import BeautifulSoup\n",
    "import unidecode\n",
    "import re\n",
    "import nltk \n",
    "import spacy\n",
    "import unidecode\n",
    "\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert text to lowercase, tokenizing , removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert text to lowercase\n",
    "input_str = 'The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.'\n",
    "input_str = input_str.lower()\n",
    "print(input_str)\n",
    "\n",
    "# removing stopwords\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "  \n",
    "word_tokens = word_tokenize(sent) \n",
    "  \n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = [] \n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "  \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "Stemming is a process of reducing words to their word stem, base or root form (for example, books — book, looked — look). The mai algorithm is Porter stemming algorithm (removes common morphological and inflexional endings from words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there\n",
      "are\n",
      "sever\n",
      "type\n",
      "of\n",
      "stem\n",
      "algorithm\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Stemming using NLTK\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "input_str='There are several types of stemming algorithms.'\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form. As opposed to stemming, lemmatization does not simply chop off inflections. Instead it uses lexical knowledge bases to get the correct base forms of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been\n",
      "had\n",
      "done\n",
      "language\n",
      "city\n",
      "mouse\n"
     ]
    }
   ],
   "source": [
    " #Lemmatization using NLTK\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str= 'been had done languages cities mice'\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging (POS)\n",
    "Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-speech tagging using TextBlob\n",
    "\n",
    "import textblob\n",
    "input_str= 'Parts of speech examples: an article, to write, interesting, easily, and, of'\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n",
      "(S\n",
      "  (NP A/DT black/JJ television/NN)\n",
      "  and/CC\n",
      "  (NP a/DT white/JJ stove/NN)\n",
      "  were/VBD\n",
      "  bought/VBN\n",
      "  for/IN\n",
      "  (NP the/DT new/JJ apartment/NN)\n",
      "  of/IN\n",
      "  John/NNP)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#  Chunking using NLTK\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "input_str= 'A black television and a white stove were bought for the new apartment of John.'\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)\n",
    "# step 2\n",
    "reg_exp = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result = rp.parse(result.tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/apple/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  Apple/NNP\n",
      "  so/IN\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Boston/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  conference/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Named-entity recognition using NLTK: \n",
    "nltk.download('words')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "input_str = 'Bill works for Apple so he went to Boston for a conference.'\n",
    "print (ne_chunk(pos_tag(word_tokenize(input_str))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using some more processes ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorem ipsum simply dummy caf printing typesetting industry\n"
     ]
    }
   ],
   "source": [
    "# input text example ...\n",
    "text = 'Lorem 456 Ipsum is !#$% simply dummy is the  café of the printing and typesetting industry.'\n",
    "\n",
    "def lower_case(text):\n",
    "    \"\"\"Converts text to lowercase\"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def remove_whitespace(text):\n",
    "    \"\"\"Removes whitespace\"\"\"\n",
    "    text = text.strip()\n",
    "    \n",
    "    return \" \".join(text.split())\n",
    "    \n",
    "def remove_appostrophe(text):\n",
    "    \"\"\"Removes apostrophe\"\"\"\n",
    "    text = re.sub(r\"'s\\b'\", '', text)\n",
    "\n",
    "    return text\n",
    "    \n",
    "def remove_parenthesis_text(text):\n",
    "    \"\"\"Removes text between parenthesis\"\"\"\n",
    "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
    "\n",
    "    return text\n",
    "    \n",
    "\n",
    "    \n",
    "def remove_special_chars( text):\n",
    "    \"\"\"Removes special characters\"\"\"\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "    return text\n",
    "    \n",
    "def remove_accented_chars( text):\n",
    "    \"\"\"remove accented characters from text, e.g. café\"\"\"\n",
    "    text = unidecode.unidecode(text)\n",
    "\n",
    "    return text\n",
    "    \n",
    "def remove_stop_words(text):\n",
    "    \"\"\"remove stop words\"\"\"\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    text = ' '.join([w for w in text.split() if not w in stop_words])\n",
    "\n",
    "    return text\n",
    "    \n",
    "def remove_short_words( text):\n",
    "    \"\"\"removes short words\"\"\"\n",
    "    text = ' '.join([w for w in text.split() if len(w) > 1])\n",
    "\n",
    "    return text\n",
    "    \n",
    "    \n",
    "def text_preprocessing( text, lower = True,contractions = True, whitespace = True, appostrophes = True,\n",
    "                      parenthesis_text = True, html_tags = False, special_chars = True, \n",
    "                      accented_chars = True, stop_words = True, short_words = True, \n",
    "                      ):\n",
    "        if lower:\n",
    "            text = lower_case(text)\n",
    "\n",
    "        if whitespace:\n",
    "            text = remove_whitespace(text)\n",
    "\n",
    "        if appostrophes:\n",
    "            text = remove_appostrophe(text)\n",
    "\n",
    "        if parenthesis_text:\n",
    "            text = remove_parenthesis_text(text)\n",
    "\n",
    "        #if html_tags:\n",
    "            #text = remove_html_tags(text)\n",
    "\n",
    "        if special_chars:\n",
    "            text = remove_special_chars(text)\n",
    "\n",
    "        if accented_chars:\n",
    "            text = remove_accented_chars(text)\n",
    "\n",
    "        if stop_words:\n",
    "            text = remove_stop_words(text)\n",
    "\n",
    "        if short_words:\n",
    "            text = remove_short_words(text)\n",
    "\n",
    "\n",
    "\n",
    "        return text\n",
    "    \n",
    "        \"\"\"\n",
    "        Returns clean text by doing \n",
    "        the following operations: \n",
    "\n",
    "        1.  Lowercase\n",
    "        2.  Remove whitespace\n",
    "        3.  Remove 's\n",
    "        4.  Remove anything between brackets\n",
    "        5.  Remove special characters\n",
    "        6.  Remove Accented words\n",
    "        7.  Remove stopwords\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "processed_data = text_preprocessing(text, lower = True,contractions = True, whitespace = True, appostrophes = True,\n",
    "                      parenthesis_text = True, html_tags = True, special_chars = True, \n",
    "                      accented_chars = True, stop_words = True, short_words = True, \n",
    "                      )\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
